{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# just performs simple automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompNode:\n",
    "    def __init__(self, tape):\n",
    "        # make sure that the gradient tape knows us\n",
    "        tape.add(self)\n",
    "        self.output = 0\n",
    "    \n",
    "    # perform the intended operation \n",
    "    # and store the result in self.output\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # assume that self.gradient has all the information \n",
    "    # from outgoing nodes prior to calling backward\n",
    "    # -> perform the local gradient step with respect to inputs\n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "    # needed to be initialized to 0 \n",
    "    def set_gradient(self, gradient):\n",
    "        self.gradient = gradient\n",
    "        \n",
    "    # receive gradients from downstream nodes     \n",
    "    def add_gradient(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    \n",
    "class ConstantNode(CompNode):\n",
    "    def __init__(self, value, tape):\n",
    "        self.value = value\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        # nothing to do here\n",
    "        pass\n",
    "    \n",
    "class Multiply(CompNode):\n",
    "    \n",
    "    def __init__(self, left : CompNode, right : CompNode, tape):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = self.left.output * self.right.output\n",
    "        \n",
    "    # has to know how to locally differentiate multiplication\n",
    "    def backward(self):\n",
    "        self.left.add_gradient(self.right.output * self.gradient)\n",
    "        self.right.add_gradient(self.left.output * self.gradient)\n",
    "        \n",
    "class Tape:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.computations = []\n",
    "        \n",
    "    def add(self, compNode : CompNode):\n",
    "        self.computations.append(compNode)\n",
    "        \n",
    "    def forward(self):\n",
    "        for compNode in self.computations:\n",
    "            compNode.forward()\n",
    "            \n",
    "    def backward(self):\n",
    "        # first initialize all gradients to zero \n",
    "        for compNode in self.computations:\n",
    "            compNode.set_gradient(0)\n",
    "            \n",
    "        # we need to invert the order    \n",
    "        self.computations.reverse()    \n",
    "        # last node gets a default value of one for the gradient\n",
    "        self.computations[0].set_gradient(1)\n",
    "        for compNode in self.computations:\n",
    "            compNode.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tape()\n",
    "a = ConstantNode(2,t)\n",
    "b = ConstantNode(3,t)\n",
    "\n",
    "o = Multiply(a, b, t)\n",
    "f = Multiply(ConstantNode(5, t), o, t)\n",
    "t.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(f.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start reverse mode autodifferentiation\n",
    "t.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "15\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# now inspect the gradients \n",
    "print(f.gradient)\n",
    "print(o.gradient)\n",
    "print(a.gradient)\n",
    "print(b.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A diamond-shaped graph that makes use of the multivariate chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tape()\n",
    "x = ConstantNode(3,t)\n",
    "y = ConstantNode(2,t)\n",
    "z = ConstantNode(1,t)\n",
    "\n",
    "h1 = Multiply(x, y, t)\n",
    "h2 = Multiply(y, z, t)\n",
    "\n",
    "h = Multiply(h1, h2, t)\n",
    "o = Multiply(h, h, t)\n",
    "t.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "--\n",
      "48\n",
      "144\n",
      "--\n",
      "96\n",
      "288\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "t.backward()\n",
    "print(h.gradient)\n",
    "print(\"--\")\n",
    "print(h1.gradient)\n",
    "print(h2.gradient)\n",
    "print(\"--\")\n",
    "print(x.gradient)\n",
    "print(y.gradient)\n",
    "print(z.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now with an explicit operation for taking the square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square(CompNode):\n",
    "    \n",
    "    def __init__(self, x : CompNode, tape : Tape):\n",
    "        self.x = x\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = self.x.output**2\n",
    "        \n",
    "    # has to know how to locally differentiate x^2\n",
    "    def backward(self):\n",
    "        self.x.add_gradient( (2*self.x.output) * self.gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tape()\n",
    "x = ConstantNode(3,t)\n",
    "y = ConstantNode(2,t)\n",
    "z = ConstantNode(1,t)\n",
    "\n",
    "h1 = Multiply(x, y, t)\n",
    "h2 = Multiply(y, z, t)\n",
    "\n",
    "h = Multiply(h1, h2, t)\n",
    "o = Square(h, t)\n",
    "t.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "--\n",
      "48\n",
      "144\n",
      "--\n",
      "96\n",
      "288\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "t.backward()\n",
    "print(h.gradient)\n",
    "print(\"--\")\n",
    "print(h1.gradient)\n",
    "print(h2.gradient)\n",
    "print(\"--\")\n",
    "print(x.gradient)\n",
    "print(y.gradient)\n",
    "print(z.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to a small neural network graph\n",
    "To really connect the dots, we need to instantiate our small framework to the problem of differentiating the loss function of a neural network. Again, this will just be a toy example with two features that are multiplied by weights, fed through a sigmoid activation and compared to a target output.\n",
    "\n",
    "<img src=\"graphfornetwork.png\" width = \"80%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need new functions for inverting a node's output, the sigmoid, and an Add operation\n",
    "class Invert(CompNode):\n",
    "    \n",
    "    def __init__(self, x : CompNode, tape : Tape):\n",
    "        self.x = x\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = (-1)*self.x.output\n",
    "        \n",
    "    # has to know how to locally differentiate x * (-1)\n",
    "    def backward(self):\n",
    "        self.x.add_gradient( (-1) * self.gradient)\n",
    "        \n",
    "class Add(CompNode):\n",
    "    \n",
    "    def __init__(self, left : CompNode, right : CompNode, tape):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = self.left.output + self.right.output\n",
    "        \n",
    "    # has to know how to locally differentiate addition (SPOILER: it just distributes its incoming gradient)\n",
    "    # d (l + r) / d l = 1 \n",
    "    # d (l + r) / d r = 1 \n",
    "    def backward(self):\n",
    "        self.left.add_gradient(self.gradient)\n",
    "        self.right.add_gradient(self.gradient)\n",
    "        \n",
    "class Sigmoid(CompNode):\n",
    "    \n",
    "    def __init__(self, x : CompNode, tape : Tape):\n",
    "        self.x = x\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = 1. / (1. + np.exp(-self.x.output))\n",
    "        \n",
    "    # has to know how to locally differentiate sigmoid (which is easy, given the output)\n",
    "    # d sigmoid(x) / d x = sigmoid(x)*(1-sigmoid(x)) \n",
    "    def backward(self):\n",
    "        local_gradient = self.output * (1. - self.output)\n",
    "        self.x.add_gradient( local_gradient * self.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to implement the graph for some input features and weights.\n",
    "<img src=\"graphfornetwork_start.png\" width = \"70%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gt = Tape()\n",
    "x1 = ConstantNode(2.,gt)\n",
    "w1 = ConstantNode(0.4,gt)\n",
    "x2 = ConstantNode(3.,gt)\n",
    "w2 = ConstantNode(-0.2,gt)\n",
    "t = ConstantNode(1.,gt)\n",
    "\n",
    "h1 = Multiply(x1, w1, gt)\n",
    "h2 = Multiply(x2, w2, gt)\n",
    "\n",
    "h = Add(h1, h2, gt)\n",
    "y = Sigmoid(h,gt)\n",
    "\n",
    "t_inv = Invert(t, gt)\n",
    "e = Add(y, t_inv, gt)\n",
    "l = Square(e, gt)\n",
    "gt.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "0.8\n",
      "-0.6000000000000001\n",
      "0.19999999999999996\n",
      "0.5498339973124778\n",
      "---\n",
      "-0.45016600268752216\n",
      "0.2026494299756622\n"
     ]
    }
   ],
   "source": [
    "print(\"---\")\n",
    "print(h1.output)\n",
    "print(h2.output)\n",
    "print(h.output)\n",
    "print(y.output)\n",
    "print(\"---\")\n",
    "print(e.output)\n",
    "print(l.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "--\n",
      "-0.9003320053750443\n",
      "-0.9003320053750443\n",
      "-0.9003320053750443\n",
      "--\n",
      "-0.22284709227322685\n",
      "--\n",
      "-0.22284709227322685\n",
      "-0.22284709227322685\n",
      "--\n",
      "-0.4456941845464537\n",
      "-0.6685412768196806\n",
      "--\n",
      "-0.08913883690929075\n",
      "0.044569418454645376\n",
      "--\n",
      "0.9003320053750443\n"
     ]
    }
   ],
   "source": [
    "gt.backward()\n",
    "print(l.gradient)\n",
    "print(\"--\")\n",
    "print(e.gradient)\n",
    "print(t_inv.gradient)\n",
    "print(y.gradient)\n",
    "print(\"--\")\n",
    "print(h.gradient)\n",
    "print(\"--\")\n",
    "print(h1.gradient)\n",
    "print(h2.gradient)\n",
    "print(\"--\")\n",
    "print(w1.gradient)\n",
    "print(w2.gradient)\n",
    "print(\"--\")\n",
    "print(x1.gradient)\n",
    "print(x2.gradient)\n",
    "print(\"--\")\n",
    "print(t.gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a test implementation of a CompNode for ReLU\n",
    "class ReLU(CompNode):\n",
    "    \n",
    "    def __init__(self, x : CompNode, tape : Tape):\n",
    "        self.x = x\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = np.max( self.x.output, 0)\n",
    "        \n",
    "    # has to know how to locally differentiate sigmoid \n",
    "\t# (which is easy, given the output)\n",
    "    # d sigmoid(x) / d x = sigmoid(x)*(1-sigmoid(x)) \n",
    "    def backward(self):\n",
    "        local_gradient = self.output * (1. - self.output)\n",
    "        self.x.add_gradient( local_gradient * self.gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
