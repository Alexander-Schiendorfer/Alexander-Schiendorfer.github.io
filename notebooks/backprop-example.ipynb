{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1./(1. + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation, by example\n",
    "\n",
    "Full explanation available [here](https://alexander-schiendorfer.github.io/2020/02/11/a-worked-example-of-backprop.html)\n",
    "Let's work this out for our small network: \n",
    "\n",
    "<img src=\"netinstance1_in.png\" width = \"80%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0 -4.0\n",
      "0.9999998874648379 0.01798620996209156\n",
      "------\n",
      "1.0044964399553609 -1.9640273550054927\n",
      "0.7319417133694889 0.12303185591001443\n"
     ]
    }
   ],
   "source": [
    "x1, x2 = 3., 1.\n",
    "w11, w21 = 6., -2.\n",
    "w12, w22 = -3., 5.\n",
    "\n",
    "h1_in = w11*x1 + w21*x2\n",
    "h2_in = w12*x1 + w22*x2\n",
    "print(h1_in, h2_in)\n",
    "h1_out, h2_out = sigmoid(h1_in), sigmoid(h2_in)\n",
    "print(h1_out, h2_out)\n",
    "\n",
    "# next layer \n",
    "print(\"------\")\n",
    "v11, v21 = 1., 0.25\n",
    "v12, v22 = -2., 2\n",
    "y1_in = v11*h1_out + v21*h2_out\n",
    "y2_in = v12*h1_out + v22*h2_out\n",
    "\n",
    "print(y1_in, y2_in)\n",
    "y1_out, y2_out = sigmoid(y1_in), sigmoid(y2_in)\n",
    "print(y1_out, y2_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"netinstance1.png\" width = \"80%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network reached `(0.73, 0.12)` whereas it should have produced `(1, 0)`. Let's zoom in on the last layer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2680582866305111 0.12303185591001443\n",
      "-0.5361165732610222 0.24606371182002887\n"
     ]
    }
   ],
   "source": [
    "t1, t2 = 1., 0.\n",
    "e1, e2 = (y1_out-t1), (y2_out-t2)\n",
    "print(e1, e2)\n",
    "\n",
    "grad_y1_out = 2*e1\n",
    "grad_y2_out = 2*e2\n",
    "print(grad_y1_out, grad_y2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10518770232556676 0.026549048699963138\n"
     ]
    }
   ],
   "source": [
    "# backprop through sigmoid, simply multiply by sigmoid(z) * (1-sigmoid(z))\n",
    "grad_y1_in = (y1_out * (1-y1_out)) * grad_y1_out\n",
    "grad_y2_in = (y2_out * (1-y2_out)) * grad_y2_out\n",
    "print(grad_y1_in, grad_y2_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes the output units:\n",
    "\n",
    "<img src=\"backprop/last-layer-intermediate.png\" width = \"95%\"> \n",
    "\n",
    "Next, we go for the weights, (here weight `v21` is highlighted)\n",
    "\n",
    "<img src=\"backprop/last-layer-weight.png\" width = \"95%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0018919280994576303 0.0004775167642113309\n",
      "-0.10518769048825163 0.02654904571226164\n"
     ]
    }
   ],
   "source": [
    "grad_v21 = grad_y1_in * h2_out\n",
    "grad_v22 = grad_y2_in * h2_out \n",
    "print(grad_v21, grad_v22)\n",
    "\n",
    "grad_v11 = grad_y1_in * h1_out\n",
    "grad_v12 = grad_y2_in * h1_out \n",
    "print(grad_v11, grad_v12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to the hidden outputs \n",
    "\n",
    "<img src=\"backprop/to-hidden.png\" width = \"95%\"> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.15828579972549303 0.026801171818534586\n"
     ]
    }
   ],
   "source": [
    "grad_h1_out = grad_y1_in*v11 + grad_y2_in*v12\n",
    "grad_h2_out = grad_y1_in*v21 + grad_y2_in*v22\n",
    "print(grad_h1_out, grad_h2_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done with the last layer and can proceed with the first layer.\n",
    "\n",
    "<img src=\"backprop/last-layer-done.png\" width = \"95%\"> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7812716122177433e-08 0.0004733812240027136\n",
      "----\n",
      "-1.7812716122177433e-08 0.0004733812240027136\n",
      "-5.34381483665323e-08 0.0014201436720081408\n",
      "-0.0014202505483048738 0.0023669417454458123\n"
     ]
    }
   ],
   "source": [
    "# backprop through sigmoid, simply multiply by sigmoid(z) * (1-sigmoid(z))\n",
    "grad_h1_in = (h1_out * (1-h1_out)) * grad_h1_out\n",
    "grad_h2_in = (h2_out * (1-h2_out)) * grad_h2_out\n",
    "print(grad_h1_in, grad_h2_in)\n",
    "print(\"----\")\n",
    "\n",
    "# get the gradients for the weights\n",
    "grad_w21 = grad_h1_in * x2\n",
    "grad_w22 = grad_h2_in * x2 \n",
    "print(grad_w21, grad_w22)\n",
    "\n",
    "grad_w11 = grad_h1_in * x1\n",
    "grad_w12 = grad_h2_in * x1 \n",
    "print(grad_w11, grad_w12)\n",
    "\n",
    "# get the gradients for the inputs (could be ignored in this case)\n",
    "grad_x1 = grad_h1_in*w11 + grad_h2_in*w12\n",
    "grad_x2 = grad_h1_in*w21 + grad_h2_in*w22\n",
    "print(grad_x1, grad_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with an autodiff framework\n",
    "\n",
    "Fortunately, we can hide away almost all of the gradient calculation behind differentiable modules (i.e., `classes`). That way, we only need to define the forward pass and a framework such as PyTorch or TensorFlow can work out the backward pass automatically.\n",
    "\n",
    "For now, we'll work with the framework described in [this post](https://alexander-schiendorfer.github.io/2020/02/16/automatic-differentiation.html) since we can inspect every line of code if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompNode:\n",
    "    def __init__(self, tape):\n",
    "        # make sure that the gradient tape knows us\n",
    "        tape.add(self)\n",
    "        self.output = 0\n",
    "    \n",
    "    # perform the intended operation \n",
    "    # and store the result in self.output\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # assume that self.gradient has all the information \n",
    "    # from outgoing nodes prior to calling backward\n",
    "    # -> perform the local gradient step with respect to inputs\n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "    # needed to be initialized to 0 \n",
    "    def set_gradient(self, gradient):\n",
    "        self.gradient = gradient\n",
    "        \n",
    "    # receive gradients from downstream nodes     \n",
    "    def add_gradient(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    \n",
    "class ConstantNode(CompNode):\n",
    "    def __init__(self, value, tape):\n",
    "        self.value = value\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        # nothing to do here\n",
    "        pass\n",
    "    \n",
    "class Multiply(CompNode):\n",
    "    \n",
    "    def __init__(self, left : CompNode, right : CompNode, tape):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = self.left.output * self.right.output\n",
    "        \n",
    "    # has to know how to locally differentiate multiplication\n",
    "    def backward(self):\n",
    "        self.left.add_gradient(self.right.output * self.gradient)\n",
    "        self.right.add_gradient(self.left.output * self.gradient)\n",
    "        \n",
    "class Tape:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.computations = []\n",
    "        \n",
    "    def add(self, compNode : CompNode):\n",
    "        self.computations.append(compNode)\n",
    "        \n",
    "    def forward(self):\n",
    "        for compNode in self.computations:\n",
    "            compNode.forward()\n",
    "            \n",
    "    def backward(self):\n",
    "        # first initialize all gradients to zero \n",
    "        for compNode in self.computations:\n",
    "            compNode.set_gradient(0)\n",
    "            \n",
    "        # we need to invert the order    \n",
    "        self.computations.reverse()    \n",
    "        # last node gets a default value of one for the gradient\n",
    "        self.computations[0].set_gradient(1)\n",
    "        for compNode in self.computations:\n",
    "            compNode.backward()\n",
    "            \n",
    "class Square(CompNode):\n",
    "    \n",
    "    def __init__(self, x : CompNode, tape : Tape):\n",
    "        self.x = x\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = self.x.output**2\n",
    "        \n",
    "    # has to know how to locally differentiate x^2\n",
    "    def backward(self):\n",
    "        self.x.add_gradient( (2*self.x.output) * self.gradient)\n",
    "\n",
    "# first, we need new functions for inverting a node's output, the sigmoid, and an Add operation\n",
    "class Invert(CompNode):\n",
    "    \n",
    "    def __init__(self, x : CompNode, tape : Tape):\n",
    "        self.x = x\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = (-1)*self.x.output\n",
    "        \n",
    "    # has to know how to locally differentiate x * (-1)\n",
    "    def backward(self):\n",
    "        self.x.add_gradient( (-1) * self.gradient)\n",
    "        \n",
    "class Add(CompNode):\n",
    "    \n",
    "    def __init__(self, left : CompNode, right : CompNode, tape):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = self.left.output + self.right.output\n",
    "        \n",
    "    # has to know how to locally differentiate addition (SPOILER: it just distributes its incoming gradient)\n",
    "    # d (l + r) / d l = 1 \n",
    "    # d (l + r) / d r = 1 \n",
    "    def backward(self):\n",
    "        self.left.add_gradient(self.gradient)\n",
    "        self.right.add_gradient(self.gradient)\n",
    "        \n",
    "class Sigmoid(CompNode):\n",
    "    \n",
    "    def __init__(self, x : CompNode, tape : Tape):\n",
    "        self.x = x\n",
    "        super().__init__(tape)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = 1. / (1. + np.exp(-self.x.output))\n",
    "        \n",
    "    # has to know how to locally differentiate sigmoid (which is easy, given the output)\n",
    "    # d sigmoid(x) / d x = sigmoid(x)*(1-sigmoid(x)) \n",
    "    def backward(self):\n",
    "        local_gradient = self.output * (1. - self.output)\n",
    "        self.x.add_gradient( local_gradient * self.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to build the forward calculations using this framework\n",
    "\n",
    "<img src=\"netinstance1.png\" width = \"85%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = Tape()\n",
    "# inputs, targets, and weights are our starting\n",
    "# points\n",
    "x1 = ConstantNode(3.,gt)\n",
    "x2 = ConstantNode(1.,gt)\n",
    "\n",
    "w11, w21 = ConstantNode(6.,gt), ConstantNode(-2.,gt)\n",
    "w12, w22 = ConstantNode(-3.,gt), ConstantNode(5.,gt)\n",
    "\n",
    "v11, v21 = ConstantNode(1.,gt), ConstantNode(0.25,gt)\n",
    "v12, v22 = ConstantNode(-2.,gt), ConstantNode(2.,gt)\n",
    "\n",
    "t1 = ConstantNode(1.,gt)\n",
    "t2 = ConstantNode(0.,gt)\n",
    "\n",
    "# calculating the hidden layer \n",
    "h1_in = Add(Multiply(x1, w11, gt), Multiply(x2, w21, gt), gt)\n",
    "h2_in = Add(Multiply(x1, w12, gt), Multiply(x2, w22, gt), gt)\n",
    "h1, h2 = Sigmoid(h1_in, gt), Sigmoid(h2_in, gt)\n",
    "\n",
    "# calculating the output layer\n",
    "y1_in = Add(Multiply(h1, v11, gt), Multiply(h2, v21, gt), gt)\n",
    "y2_in = Add(Multiply(h1, v12, gt), Multiply(h2, v22, gt), gt)\n",
    "y1, y2 = Sigmoid(y1_in, gt), Sigmoid(y2_in, gt)\n",
    "\n",
    "t1_inv = Invert(t1, gt)\n",
    "t2_inv = Invert(t2, gt)\n",
    "\n",
    "e1 = Add(y1, t1_inv, gt)\n",
    "e2 = Add(y2, t2_inv, gt)\n",
    "\n",
    "l = Add(Square(e1, gt), Square(e2,gt), gt)\n",
    "gt.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer gradients by framework\n",
      "-5.34381483665323e-08 0.0014201436720081408\n",
      "-1.7812716122177433e-08 0.0004733812240027136\n",
      "--\n",
      "First layer gradients manually\n",
      "-5.34381483665323e-08 0.0014201436720081408\n",
      "-1.7812716122177433e-08 0.0004733812240027136\n"
     ]
    }
   ],
   "source": [
    "# now we can just play it backwards and inspect the results\n",
    "gt.backward()\n",
    "\n",
    "print(\"First layer gradients by framework\")\n",
    "print(w11.gradient, w12.gradient)\n",
    "print(w21.gradient, w22.gradient)\n",
    "print(\"--\")\n",
    "\n",
    "print(\"First layer gradients manually\")\n",
    "print(grad_w11, grad_w12)\n",
    "print(grad_w21, grad_w22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's work out that example using PyTorch\n",
    "\n",
    "<img src=\"backprop/PyTorch-logo.jpg\" width = \"85%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x1 = torch.tensor(3., requires_grad=False)\n",
    "x2 = torch.tensor(1., requires_grad=False)\n",
    "\n",
    "w11 = torch.tensor(6., requires_grad=True)\n",
    "w21 = torch.tensor(-2., requires_grad=True)\n",
    "w12 = torch.tensor(-3., requires_grad=True)\n",
    "w22 = torch.tensor(5., requires_grad=True)\n",
    "\n",
    "v11 = torch.tensor(1., requires_grad=True)\n",
    "v21 = torch.tensor(0.25, requires_grad=True)\n",
    "v12 = torch.tensor(-2., requires_grad=True)\n",
    "v22 = torch.tensor(2., requires_grad=True)\n",
    "\n",
    "t1 = torch.tensor(1., requires_grad=False)\n",
    "t2 = torch.tensor(0., requires_grad=False)\n",
    "\n",
    "# calculating the hidden layer \n",
    "h1 = torch.sigmoid(w11*x1 + w21*x2)\n",
    "h2 = torch.sigmoid(w12*x1 + w22*x2)\n",
    "\n",
    "# calculating the output layer\n",
    "y1 = torch.sigmoid(v11*h1 + v21*h2)\n",
    "y2 = torch.sigmoid(v12*h1 + v22*h2)\n",
    "\n",
    "e1 = y1 - t1\n",
    "e2 = y2 - t2\n",
    "\n",
    "# the loss function \n",
    "l = e1**2 + e2**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer gradients by framework\n",
      "tensor(-5.6607e-08) tensor(0.0014)\n",
      "tensor(-1.8869e-08) tensor(0.0005)\n",
      "--\n",
      "First layer gradients manually\n",
      "-5.34381483665323e-08 0.0014201436720081408\n",
      "-1.7812716122177433e-08 0.0004733812240027136\n"
     ]
    }
   ],
   "source": [
    "l.backward()\n",
    "\n",
    "print(\"First layer gradients by framework\")\n",
    "print(w11.grad, w12.grad)\n",
    "print(w21.grad, w22.grad)\n",
    "print(\"--\")\n",
    "\n",
    "print(\"First layer gradients manually\")\n",
    "print(grad_w11, grad_w12)\n",
    "print(grad_w21, grad_w22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with higher precision \n",
    "import torch\n",
    "x1 = torch.tensor(3., requires_grad=False,dtype=torch.float64)\n",
    "x2 = torch.tensor(1., requires_grad=False,dtype=torch.float64)\n",
    "\n",
    "w11 = torch.tensor(6., requires_grad=True,dtype=torch.float64)\n",
    "w21 = torch.tensor(-2., requires_grad=True,dtype=torch.float64)\n",
    "w12 = torch.tensor(-3., requires_grad=True,dtype=torch.float64)\n",
    "w22 = torch.tensor(5., requires_grad=True,dtype=torch.float64)\n",
    "\n",
    "v11 = torch.tensor(1., requires_grad=True,dtype=torch.float64)\n",
    "v21 = torch.tensor(0.25, requires_grad=True,dtype=torch.float64)\n",
    "v12 = torch.tensor(-2., requires_grad=True,dtype=torch.float64)\n",
    "v22 = torch.tensor(2., requires_grad=True,dtype=torch.float64)\n",
    "\n",
    "t1 = torch.tensor(1., requires_grad=False,dtype=torch.float64)\n",
    "t2 = torch.tensor(0., requires_grad=False,dtype=torch.float64)\n",
    "\n",
    "# calculating the hidden layer \n",
    "h1 = torch.sigmoid(w11*x1 + w21*x2)\n",
    "h2 = torch.sigmoid(w12*x1 + w22*x2)\n",
    "\n",
    "# calculating the output layer\n",
    "y1 = torch.sigmoid(v11*h1 + v21*h2)\n",
    "y2 = torch.sigmoid(v12*h1 + v22*h2)\n",
    "\n",
    "e1 = y1 - t1\n",
    "e2 = y2 - t2\n",
    "\n",
    "# the loss function \n",
    "l = e1**2 + e2**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer gradients by framework\n",
      "tensor(-5.3438e-08, dtype=torch.float64) tensor(0.0014, dtype=torch.float64)\n",
      "tensor(-1.7813e-08, dtype=torch.float64) tensor(0.0005, dtype=torch.float64)\n",
      "--\n",
      "First layer gradients manually\n",
      "-5.34381483665323e-08 0.0014201436720081408\n",
      "-1.7812716122177433e-08 0.0004733812240027136\n"
     ]
    }
   ],
   "source": [
    "l.backward()\n",
    "\n",
    "print(\"First layer gradients by framework\")\n",
    "print(w11.grad, w12.grad)\n",
    "print(w21.grad, w22.grad)\n",
    "print(\"--\")\n",
    "\n",
    "print(\"First layer gradients manually\")\n",
    "print(grad_w11, grad_w12)\n",
    "print(grad_w21, grad_w22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A vectorized implementation \n",
    "\n",
    "Where $X$ contains a row for every training instance, $W$ is a weight matrix, and activations are applied componentwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73194171 0.12303186]]\n"
     ]
    }
   ],
   "source": [
    "# first, the training data \n",
    "X = np.array( [[3., 1. ]])\n",
    "T = np.array( [[1., 0. ]])\n",
    "\n",
    "# then the weights \n",
    "W = np.array([[6., -3.], [-2., 5.]])\n",
    "V = np.array([[1., -2.], [0.25, 2.]])\n",
    "\n",
    "# now the forward pass \n",
    "H_in = np.dot(X,W)\n",
    "H = sigmoid(H_in)\n",
    "# ---- \n",
    "Y_in = np.dot(H,V)\n",
    "Y = sigmoid(Y_in)\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08699208]]\n"
     ]
    }
   ],
   "source": [
    "# now for the loss function per training instance\n",
    "# simply apply componentwise subtraction\n",
    "E = Y - T \n",
    "# square each term\n",
    "E_sq = E**2\n",
    "# sum up per row (keep dimensions as they are)\n",
    "L = np.sum(E_sq, axis=1, keepdims=True)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1051877   0.02654905]]\n"
     ]
    }
   ],
   "source": [
    "grad_Y = 2*E\n",
    "grad_Y_in = (Y) * (1-Y) * grad_Y\n",
    "print(grad_Y_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now for the weights:\n",
    "\n",
    "<img src=\"backprop/last-layer-weight.png\" width = \"95%\"> \n",
    "\n",
    "Let's generalize this to get an outer product:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{ \\partial v_{i,j}} = \\frac{\\partial L}{\\partial y_j^{(\\mathit{in})}} \\cdot \n",
    "\\underbrace{ \\frac{\\partial y_j^{(\\mathit{in})}}{\\partial v_{i,j}} }_{h_i}\n",
    "$$\n",
    "\n",
    "<img src=\"backprop/grad_V.png\" width = \"40%\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10518769  0.02654905]\n",
      " [-0.00189193  0.00047752]]\n"
     ]
    }
   ],
   "source": [
    "grad_V = np.dot(H.T, grad_Y_in)\n",
    "print(grad_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now again back to the hidden outputs \n",
    "\n",
    "<img src=\"backprop/to-hidden.png\" width = \"95%\"> \n",
    "\n",
    "in vector notation \n",
    "\n",
    "<img src=\"backprop/grad_h.png\" width = \"95%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1582858   0.02680117]]\n"
     ]
    }
   ],
   "source": [
    "grad_H = np.dot(grad_Y_in, V.T)\n",
    "print(grad_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.34381484e-08  1.42014367e-03]\n",
      " [-1.78127161e-08  4.73381224e-04]]\n"
     ]
    }
   ],
   "source": [
    "# now on to the hidden layer\n",
    "grad_H_in = (H * (1.-H))*grad_H # sigmoid\n",
    "grad_W = np.dot(X.T, grad_H_in) # outer product\n",
    "grad_X = np.dot(grad_H_in, W.T) # not really necessary\n",
    "print(grad_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network class\n",
    "\n",
    "Time to wrap this all up into a nice class that performs forward and backward pass and can be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1./(1. + np.exp(-z))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim=2, hidden_dim=2, output_dim=2):\n",
    "        self.W = 0.1 * np.random.rand(input_dim, hidden_dim)\n",
    "        self.V = 0.1 * np.random.rand(hidden_dim, output_dim)\n",
    "                \n",
    "    # expects X to be a (n X input_dim) matrix\n",
    "    def forward(self, X):\n",
    "        self.X = X # keep for backward pass \n",
    "        \n",
    "        self.H_in = np.dot(X, self.W)\n",
    "        self.H = sigmoid(self.H_in)\n",
    "        # ---- \n",
    "        self.Y_in = np.dot(self.H, self.V)\n",
    "        self.Y = sigmoid(self.Y_in)\n",
    "        return self.Y\n",
    "    \n",
    "    # expects T to be a (n X output_dim) matrix \n",
    "    def backward(self, T):\n",
    "        E = self.Y - T \n",
    "        E_sq = E**2\n",
    "        self.L = np.sum(E_sq, axis=1, keepdims=True)\n",
    "        grad_Y = 2*E\n",
    "        \n",
    "        # -----\n",
    "        grad_Y_in = (self.Y) * (1-self.Y) * grad_Y # sigmoid\n",
    "        grad_V = np.dot(self.H.T, grad_Y_in) # outer product\n",
    "        grad_H = np.dot(grad_Y_in, self.V.T)\n",
    "        \n",
    "        # -----\n",
    "        grad_H_in = (self.H * (1.-self.H))*grad_H # sigmoid\n",
    "        grad_W = np.dot(self.X.T, grad_H_in) # outer product\n",
    "        return grad_W, grad_V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.34381484e-08  1.42014367e-03]\n",
      " [-1.78127161e-08  4.73381224e-04]]\n",
      "[[-0.10518769  0.02654905]\n",
      " [-0.00189193  0.00047752]]\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork()\n",
    "net.W = W\n",
    "net.V = V\n",
    "net.forward(X)\n",
    "g_W, g_V = net.backward(T)\n",
    "print(g_W)\n",
    "print(g_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying gradients for weight updates \n",
    "\n",
    "Let's now change some weights to improve the network's performance! Recall the training data\n",
    "\n",
    "<img src=\"backprop/trainingdata.png\" width = \"60%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()\n",
    "\n",
    "net.W = W.copy()\n",
    "net.V = V.copy()\n",
    "\n",
    "# first training instance\n",
    "X = np.array( [[3., 1. ]])\n",
    "T = np.array( [[1., 0. ]])\n",
    "net.forward(X)\n",
    "g_W_1, g_V_1 = net.backward(T)\n",
    "# initial loss\n",
    "init_loss_1 = net.L\n",
    "\n",
    "# second training instance\n",
    "X = np.array( [[-1., 4. ]])\n",
    "T = np.array( [[0., 1. ]])\n",
    "net.forward(X)\n",
    "g_W_2, g_V_2 = net.backward(T)\n",
    "# initial loss\n",
    "init_loss_2 = net.L\n",
    "\n",
    "g_W, g_V = g_W_1 + g_W_2, g_V_1 + g_V_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.00000016 -3.00071007]\n",
      " [-2.00000053  4.99976331]]\n",
      "[[ 1.05259373 -2.01327451]\n",
      " [ 0.11257513  2.01227682]]\n"
     ]
    }
   ],
   "source": [
    "# update weights\n",
    "alpha = 0.5\n",
    "\n",
    "net.W -= alpha * g_W\n",
    "net.V -= alpha * g_V\n",
    "\n",
    "print(net.W)\n",
    "print(net.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74165987 0.12162137]]\n",
      "Old loss for instance #1: [[0.08699208]]\n",
      "New loss for instance #1: [[0.08153138]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array( [[3., 1. ]])\n",
    "T = np.array( [[1., 0. ]])\n",
    "y = net.forward(X)\n",
    "print(y)\n",
    "\n",
    "net.backward(T)\n",
    "print(\"Old loss for instance #1:\", init_loss_1)\n",
    "print(\"New loss for instance #1:\", net.L)\n",
    "new_loss_1 = net.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52811432 0.88207988]]\n",
      "Old loss for instance #2: [[0.33025203]]\n",
      "New loss for instance #2: [[0.29280989]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array( [[-1., 4. ]])\n",
    "T = np.array( [[0., 1. ]])\n",
    "y = net.forward(X)\n",
    "print(y)\n",
    "\n",
    "net.backward(T)\n",
    "print(\"Old loss for instance #2:\", init_loss_2)\n",
    "print(\"New loss for instance #2:\", net.L)\n",
    "new_loss_2 = net.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.34381484e-08  1.42014367e-03]\n",
      " [-1.78127161e-08  4.73381224e-04]]\n"
     ]
    }
   ],
   "source": [
    "# vanishing gradients \n",
    "print(grad_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41724411]]\n",
      "[[0.37434126]]\n"
     ]
    }
   ],
   "source": [
    "print(init_loss_1+init_loss_2)\n",
    "print(new_loss_1+new_loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94621368 0.04892312]\n",
      "[0.05590822 0.95093023]\n"
     ]
    }
   ],
   "source": [
    "# iterate for 200 epochs\n",
    "train_X = np.array( [[3., 1. ], [-1., 4.]])\n",
    "train_T = np.array( [[1., 0. ], [0., 1.]])\n",
    "\n",
    "n_epochs = 200\n",
    "alpha = 0.5\n",
    "\n",
    "for n in range(n_epochs):\n",
    "    # grad_W\n",
    "    grad_W = np.zeros_like(net.W)\n",
    "    grad_V = np.zeros_like(net.V)\n",
    "    for i in range(train_X.shape[0]):\n",
    "        X = train_X[i, :].reshape(1,-1)\n",
    "        T = train_T[i, :].reshape(1,-1)\n",
    "        \n",
    "        net.forward(X)\n",
    "        grad_W_i, grad_V_i = net.backward(T)\n",
    "        grad_W += grad_W_i\n",
    "        grad_V += grad_V_i\n",
    "    \n",
    "    # apply gradient \n",
    "    net.W -= alpha * grad_W\n",
    "    net.V -= alpha * grad_V\n",
    "    \n",
    "# inspect the trained net's outputs\n",
    "print(net.forward(np.array([3.,1.])))\n",
    "print(net.forward(np.array([-1.,4.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.00000866 -3.2025047 ]\n",
      " [-2.00002564  4.93249844]]\n",
      "[[ 2.89356441 -2.99473202]\n",
      " [-2.82651445  2.96419993]]\n"
     ]
    }
   ],
   "source": [
    "print(net.W)\n",
    "print(net.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
